{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA/SARIMA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Use 'Adj.Close' for ARIMA\n",
    "data = df['Adj.Close'].values\n",
    "train_data = data[:int(len(data) * 0.8)]\n",
    "test_data = data[int(len(data) * 0.8):]\n",
    "\n",
    "# Fit the ARIMA model\n",
    "arima_model = ARIMA(train_data, order=(5,1,0))  # p, d, q values should be optimized\n",
    "arima_results = arima_model.fit()\n",
    "\n",
    "# Make predictions\n",
    "arima_predictions = arima_results.forecast(steps=len(test_data))\n",
    "\n",
    "# Evaluate the model\n",
    "arima_mse = mean_squared_error(test_data, arima_predictions)\n",
    "arima_mae = mean_absolute_error(test_data, arima_predictions)\n",
    "\n",
    "print(f'ARIMA MSE: {arima_mse}')\n",
    "print(f'ARIMA MAE: {arima_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ed466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ETS\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Prepare the data\n",
    "data = df['Adj.Close'].values\n",
    "train_data = data[:int(len(data) * 0.8)]\n",
    "test_data = data[int(len(data) * 0.8):]\n",
    "\n",
    "# Fit the ETS model\n",
    "ets_model = ExponentialSmoothing(train_data, trend='add', seasonal='mul', seasonal_periods=12).fit()\n",
    "\n",
    "# Forecast the next steps\n",
    "ets_forecast = ets_model.forecast(steps=len(test_data))\n",
    "\n",
    "# Evaluate the model\n",
    "ets_mse = mean_squared_error(test_data, ets_forecast)\n",
    "ets_mae = mean_absolute_error(test_data, ets_forecast)\n",
    "\n",
    "print(f'ETS MSE: {ets_mse}')\n",
    "print(f'ETS MAE: {ets_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75453e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GARCH \n",
    "import pandas as pd\n",
    "from arch import arch_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Prepare the data\n",
    "returns = 100 * df['Adj.Close'].pct_change().dropna().values\n",
    "train_data = returns[:int(len(returns) * 0.8)]\n",
    "test_data = returns[int(len(returns) * 0.8):]\n",
    "\n",
    "# Fit the GARCH model\n",
    "garch = arch_model(train_data, p=1, q=1)\n",
    "model = garch.fit(disp='off')\n",
    "\n",
    "# Forecast the test set\n",
    "test_predictions = model.forecast(horizon=len(test_data)).mean.dropna().values\n",
    "\n",
    "# Since GARCH is a volatility model, we directly compare the forecasted variance with the true variance\n",
    "garch_mse = mean_squared_error(test_data**2, test_predictions**2)\n",
    "garch_mae = mean_absolute_error(test_data**2, test_predictions**2)\n",
    "\n",
    "print(f'GARCH MSE: {garch_mse}')\n",
    "print(f'GARCH MAE: {garch_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31236e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Select features for SVM - we'll use the same features as for BiLSTM for comparison\n",
    "data = df[['Adj.Close', 'Volume', 'SMA_50', 'GSPC.Adjusted', 'rsi']].values\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data\n",
    "training_data_len = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:training_data_len]\n",
    "test_data = scaled_data[training_data_len:]\n",
    "\n",
    "# Prepare the data for SVM\n",
    "x_train, y_train = train_data[:, 1:], train_data[:, 0]\n",
    "x_test, y_test = test_data[:, 1:], test_data[:, 0]\n",
    "\n",
    "# Initialize and fit the SVR model\n",
    "svm_model = SVR(kernel='rbf')\n",
    "svm_model.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "svm_predictions = svm_model.predict(x_test)\n",
    "\n",
    "# Scale back the predictions to the original range\n",
    "svm_predictions = scaler.inverse_transform(np.column_stack((svm_predictions, np.zeros(svm_predictions.shape[0],4))))\n",
    "\n",
    "# Evaluate the model\n",
    "svm_mse = mean_squared_error(y_test, svm_predictions[:,0])\n",
    "svm_mae = mean_absolute_error(y_test, svm_predictions[:,0])\n",
    "\n",
    "print(f'SVM MSE: {svm_mse}')\n",
    "print(f'SVM MAE: {svm_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16950e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "X = df[['Volume', 'SMA_50', 'GSPC.Adjusted', 'rsi']].values\n",
    "y = df['Adj.Close'].values\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Fit the Random Forest Regressor\n",
    "rfr_model = RandomForestRegressor(n_estimators=100)\n",
    "rfr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rfr_predictions = rfr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rfr_mse = mean_squared_error(y_test, rfr_predictions)\n",
    "rfr_mae = mean_absolute_error(y_test, rfr_predictions)\n",
    "\n",
    "print(f'RFR MSE: {rfr_mse}')\n",
    "print(f'RFR MAE: {rfr_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac85f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "X = df[['Volume', 'SMA_50', 'GSPC.Adjusted', 'rsi']].values\n",
    "y = df['Adj.Close'].values\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Fit the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "xgb_mse = mean_squared_error(y_test, xgb_predictions)\n",
    "xgb_mae = mean_absolute_error(y_test, xgb_predictions)\n",
    "\n",
    "print(f'XGB MSE: {xgb_mse}')\n",
    "print(f'XGB MAE: {xgb_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c5c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Select features\n",
    "data = df[['Adj.Close', 'Volume', 'SMA_50', 'GSPC.Adjusted', 'rsi']].values\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data\n",
    "training_data_len = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:training_data_len]\n",
    "test_data = scaled_data[training_data_len:]\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "sequence_length = 60\n",
    "x_train, y_train = [], []\n",
    "for i in range(sequence_length, len(train_data)):\n",
    "    x_train.append(train_data[i-sequence_length:i, :])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Initialize and fit the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "lstm_model.add(LSTM(50, return_sequences=False))\n",
    "lstm_model.add(Dense(25))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Prepare the test data\n",
    "x_test, y_test = [], []\n",
    "for i in range(sequence_length, len(test_data)):\n",
    "    x_test.append(test_data[i-sequence_length:i, :])\n",
    "y_test = test_data[sequence_length:, 0]\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Make predictions\n",
    "lstm_predictions = lstm_model.predict(x_test)\n",
    "\n",
    "# Scale back the predictions to the original range\n",
    "lstm_predictions = scaler.inverse_transform(np.column_stack((lstm_predictions, np.zeros(lstm_predictions.shape[0],4))))\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_mse = mean_squared_error(y_test, lstm_predictions[:,0])\n",
    "lstm_mae = mean_absolute_error(y_test, lstm_predictions[:,0])\n",
    "\n",
    "print(f'LSTM MSE: {lstm_mse}')\n",
    "print(f'LSTM MAE: {lstm_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvLSTM \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Select features\n",
    "data = df['Adj.Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data\n",
    "training_data_len = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:training_data_len]\n",
    "test_data = scaled_data[training_data_len - 60:]\n",
    "\n",
    "# Prepare the data for ConvLSTM\n",
    "sequence_length = 60\n",
    "x_train, y_train = [], []\n",
    "for i in range(sequence_length, len(train_data)):\n",
    "    x_train.append(train_data[i - sequence_length:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1, 1, 1)\n",
    "\n",
    "# Initialize and fit the ConvLSTM model\n",
    "convlstm_model = Sequential()\n",
    "convlstm_model.add(ConvLSTM2D(filters=64, kernel_size=(1, 1), activation='relu', input_shape=(sequence_length, 1, 1, 1)))\n",
    "convlstm_model.add(Flatten())\n",
    "convlstm_model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "convlstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "convlstm_model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Prepare the test data\n",
    "x_test, y_test = [], []\n",
    "for i in range(sequence_length, len(test_data)):\n",
    "    x_test.append(test_data[i - sequence_length:i, 0])\n",
    "\n",
    "x_test, y_test = np.array(x_test), np.array(y_test)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1, 1, 1)\n",
    "\n",
    "# Make predictions\n",
    "convlstm_predictions = convlstm_model.predict(x_test)\n",
    "\n",
    "# Scale back the predictions to the original range\n",
    "convlstm_predictions = scaler.inverse_transform(convlstm_predictions)\n",
    "\n",
    "# Evaluate the model\n",
    "convlstm_mse = mean_squared_error(y_test, convlstm_predictions)\n",
    "convlstm_mae = mean_absolute_error(y_test, convlstm_predictions)\n",
    "\n",
    "print(f'ConvLSTM MSE: {convlstm_mse}')\n",
    "print(f'ConvLSTM MAE: {convlstm_mae}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc577fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvLSTM2D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('AAPLenhanced.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "df.fillna(method='pad', inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Select features\n",
    "data = df['Adj.Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data\n",
    "training_data_len = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:training_data_len]\n",
    "test_data = scaled_data[training_data_len - 60:]\n",
    "\n",
    "# Prepare the data for ConvLSTM\n",
    "sequence_length = 60\n",
    "x_train, y_train = [], []\n",
    "for i in range(sequence_length, len(train_data)):\n",
    "    x_train.append(train_data[i - sequence_length:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1, 1, 1)\n",
    "\n",
    "# Initialize and fit the ConvLSTM2D model\n",
    "convlstm2d_model = Sequential()\n",
    "convlstm2d_model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(sequence_length, rows, cols, 1), return_sequences=True))\n",
    "convlstm2d_model.add(ConvLSTM2D(filters=32, kernel_size=(3, 3), activation='relu', return_sequences=False))\n",
    "convlstm2d_model.add(Flatten())\n",
    "convlstm2d_model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "convlstm2d_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "convlstm2d_model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Make predictions\n",
    "convlstm2d_predictions = convlstm2d_model.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "convlstm2d_mse = mean_squared_error(y_test, convlstm2d_predictions)\n",
    "convlstm2d_mae = mean_absolute_error(y_test, convlstm2d_predictions)\n",
    "\n",
    "print(f'ConvLSTM2D MSE: {convlstm2d_mse}')\n",
    "print(f'ConvLSTM2D MAE: {convlstm2d_mae}')\n"
   ]
  },
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
